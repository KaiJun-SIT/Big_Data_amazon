{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ce108-dde5-477b-ade3-077c806884d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 22713\n",
      "Schema:\n",
      "+-------------+-----------------+--------------------------------------------------------------------+------------------+-----------------------+------------+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|product/price|product/productId|product/title                                                       |review/helpfulness|review/profileName     |review/score|review/summary                     |review/text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |review/time|review/userId |\n",
      "+-------------+-----------------+--------------------------------------------------------------------+------------------+-----------------------+------------+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|294.95       |B000PDESIM       |Momentum Men's 1M-DV50S1 M50 DSS Silver Dial Black Rubber Dive Watch|1/1               |D. Berry II            |5.0         |Tough, Good Looking Watch!         |I purchased a Momentum M1 diver's watch for my wife 3 years ago. She wears it daily. It has kept perfect time for 3 years and still looks as new as the day it came out of the box. I have had 3 watches in the past 5 years, they were all meeting the same fate- death by swimming pool despite the 'waterproof' claims. After my most recent watch death, I'd had enough. I purchased this watch based on my experience with my wife's Momentum. This was done despite the fact that I wasn't thrilled with the look of the watch from the internet pictures. I just didn't think the looks were that great. In person this watch looks MUCH better. It is a VERY cool looking watch. It is the PERFECT size for my wrist. Neither too big or too small. It is built like a tank. Did they cut this thing out of a block of stainless steel? Despite the solid nature it is not too heavy, although it is heavier than the average watch. It is not annoyingly heavy, I don't mind it at all. This watch has a reported 10 year battery, which is important. Every time you replace the battery, if the watch isn't sealed properly it will lose it's 'waterpoofness'. Better to not HAVE to replace the battery at all. It comes in three face color options (Blue, Black, or this one - 'Silver') with 5 band choices. It has a screw down crown and will dive deeper safely than I can. It has a sapphire crystal that should be kept away from diamonds to avoid scratching, but nothing else should. I suspect this watch may outlive me. It is that tough.|1309651200 |AZMDNMPDWLLI3 |\n",
      "|99.99        |B0007OEQZO       |Citizen Women's EG2280-51E Eco-Drive Charm Bracelet Watch           |0/1               |Jay Sharma \"Jay Sharma\"|5.0         |$265 watch at $65!!! its a steal :)|U cant get such a watch at much cheaper price. Its a nice watch from citizen with eco-drive technology, that means u need not to worry about running out of batteries. Moreover it is vry lite in weight. Though i hav purchased this watch for gifting it to someone but m sure that she will like it for sure (& y not the watch comes with the girl's best friend - Diamonds).In short, its a nice bracelet with a watch attached to it :)                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      |1190937600 |A2WRNZRR891JYA|\n",
      "|99.99        |B0007OEQZO       |Citizen Women's EG2280-51E Eco-Drive Charm Bracelet Watch           |0/1               |W. YUE                 |5.0         |cute watch                         |The watch is nice and cute, i love it. but the shipping fee is a little higher. It cost me 11 bucks~~                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              |1188432000 |A29H29DRLPLQHS|\n",
      "|99.99        |B0007OEQZO       |Citizen Women's EG2280-51E Eco-Drive Charm Bracelet Watch           |8/8               |Neko \"Nakamura\"        |5.0         |What a gorgeous watch!!! (for $65) |I have been looking for a new dress watch for a month. (I am kind of a fussy person). I saw this watch and read the reviews . It made me considered about buying it. I decided to buy this one. I've got it today. I thought of two words on my mind. (everytime when I bought stuff online) \"Satisfied\" or \"Disappointed\". After I opened the package. I smiled all day long.Nice watch!!! Good price.!A few things want to mention you all, The watch is a little bit smaller than I expected (But it's not a promblem because my wrist it's so small(6\").it fits me perfectly)Don't be panic about the watch doesn't work at the first time. You have to let it charges for a while.(Good function,though! you don't need to worry about changing baterries)I love the lobster claw-clasp because I can adjust it myself. (I always got a problem about paying extra for adjusting the band)Well..I'd recommend this watch to anybody. Buy for yourself or as a gift. You will be happy anyway.Love it!!                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        |1191542400 |A3P3IZ23ULHRFW|\n",
      "|99.99        |B0007OEQZO       |Citizen Women's EG2280-51E Eco-Drive Charm Bracelet Watch           |5/6               |Yogesh Dhingra         |5.0         |Beautiful Eco-Drive watch          |It's a nice looking watch, Normally I check watches in any store to get the look and feel before ordering online, This wasn't available in any store, I still ordered it and it looks moer beautiful than it's in the picture.But 1 thing is Citizen has stopped manfacturing this particular model.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               |1186358400 |A8VPWEUHAUDMU |\n",
      "+-------------+-----------------+--------------------------------------------------------------------+------------------+-----------------------+------------+-----------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews Analysis\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "# Now you can read data from HDFS\n",
    "# Replace the path with your actual path where JSON files are stored\n",
    "df = spark.read.json(\"hdfs://namenode:9000/user/hadoop/amazon_reviews/data/filtered_data/Watches.filtered.json\")\n",
    "\n",
    "# Verify that data was loaded correctly\n",
    "print(f\"Number of records: {df.count()}\")\n",
    "print(\"Schema:\")\n",
    "#df.printSchema()\n",
    "\n",
    "# Show a few sample records\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c0406ab-119c-4685-be4f-96f34c6a70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Spark!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark with connection to your cluster and HDFS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews Topic Modeling\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Connected to Spark!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d642094-cf5a-4bd7-bc3b-b86ccf65a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Amazon_Instant_Video, Records: 102103\n",
      "Category: Arts, Records: 18313\n",
      "Category: Automotive, Records: 122187\n",
      "Category: Baby, Records: 60774\n",
      "Category: Beauty, Records: 119483\n",
      "Category: Books, Records: 1126634\n",
      "Category: Cell_Phones_&_Accessories, Records: 23723\n",
      "Category: Clothing_&_Accessories, Records: 10758\n",
      "Category: Electronics, Records: 340186\n",
      "Category: Gourmet_Foods, Records: 97891\n",
      "Category: Health, Records: 211388\n",
      "Category: Home_&_Kitchen, Records: 409426\n",
      "Category: Industrial_&_Scientific, Records: 25506\n",
      "Category: Jewelry, Records: 22862\n",
      "Category: Kindle_Store, Records: 11\n",
      "Category: Movies_&_TV, Records: 732723\n",
      "Category: Music, Records: 689520\n",
      "Category: Musical_Instruments, Records: 54373\n",
      "Category: Office_Products, Records: 65053\n",
      "Category: Patio, Records: 115932\n",
      "Category: Pet_Supplies, Records: 117584\n",
      "Category: Shoes, Records: 3581\n",
      "Category: Software, Records: 25999\n",
      "Category: Sports_&_Outdoors, Records: 196970\n",
      "Category: Tools_&_Home_Improvement, Records: 231206\n",
      "Category: Toys_&_Games, Records: 211452\n",
      "Category: Video_Games, Records: 156810\n",
      "Category: Watches, Records: 22713\n",
      "Total categories loaded: 28\n",
      "Total reviews: 5315161\n"
     ]
    }
   ],
   "source": [
    "# Adjust path to match your actual location in HDFS\n",
    "base_path = \"hdfs://namenode:9000/user/hadoop/amazon_reviews/data/filtered_data/\"\n",
    "\n",
    "# Get a count of reviews per category to understand data distribution\n",
    "category_counts = []\n",
    "\n",
    "# List all categories you have\n",
    "categories = [\n",
    "    \"Amazon_Instant_Video\",\n",
    "    \"Arts\",\n",
    "    \"Automotive\",\n",
    "    \"Baby\",\n",
    "    \"Beauty\",\n",
    "    \"Books\",\n",
    "    \"Cell_Phones_&_Accessories\",\n",
    "    \"Clothing_&_Accessories\",\n",
    "    \"Electronics\",\n",
    "    \"Gourmet_Foods\",\n",
    "    \"Health\",\n",
    "    \"Home_&_Kitchen\",\n",
    "    \"Industrial_&_Scientific\",\n",
    "    \"Jewelry\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Movies_&_TV\",\n",
    "    \"Music\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio\",\n",
    "    \"Pet_Supplies\",\n",
    "    \"Shoes\",\n",
    "    \"Software\",\n",
    "    \"Sports_&_Outdoors\",\n",
    "    \"Tools_&_Home_Improvement\",\n",
    "    \"Toys_&_Games\",\n",
    "    \"Video_Games\",\n",
    "    \"Watches\"\n",
    "]\n",
    "\n",
    "# Count records per category\n",
    "for category in categories:\n",
    "    try:\n",
    "        df = spark.read.json(f\"{base_path}{category}.filtered.json\")\n",
    "        count = df.count()\n",
    "        category_counts.append((category, count))\n",
    "        print(f\"Category: {category}, Records: {count}\")\n",
    "    except:\n",
    "        print(f\"Error loading category: {category}\")\n",
    "\n",
    "# Check total number of reviews\n",
    "print(f\"Total categories loaded: {len(category_counts)}\")\n",
    "print(f\"Total reviews: {sum([count for _, count in category_counts])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e2981dd-aa62-427d-9949-ea86cf43c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon_Instant_Video...\n",
      "Loading Arts...\n",
      "Loading Automotive...\n",
      "Loading Baby...\n",
      "Loading Beauty...\n",
      "Loading Books...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Cell_Phones_&_Accessories...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m all_df\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load data from all categories (this might take a while)\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m all_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mload_categories\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal reviews loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_reviews\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 14\u001b[0m, in \u001b[0;36mload_categories\u001b[0;34m(categories, base_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m     df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mjson(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.filtered.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m     df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mlit(category))\n\u001b[0;32m---> 14\u001b[0m     all_df \u001b[38;5;241m=\u001b[39m \u001b[43mall_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcategory\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py:3923\u001b[0m, in \u001b[0;36mDataFrame.union\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m   3827\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21munion\u001b[39m(\u001b[38;5;28mself\u001b[39m, other: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataFrame\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   3828\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a new :class:`DataFrame` containing the union of rows in this and another\u001b[39;00m\n\u001b[1;32m   3829\u001b[0m \u001b[38;5;124;03m    :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   3830\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3921\u001b[0m \u001b[38;5;124;03m    +---+-----+\u001b[39;00m\n\u001b[1;32m   3922\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Function to load all categories\n",
    "def load_categories(categories, base_path):\n",
    "    # Start with the first category\n",
    "    print(f\"Loading {categories[0]}...\")\n",
    "    all_df = spark.read.json(f\"{base_path}{categories[0]}.filtered.json\")\n",
    "    all_df = all_df.withColumn(\"category\", F.lit(categories[0]))\n",
    "    \n",
    "    # Add all other categories with a union\n",
    "    for category in categories[1:]:\n",
    "        try:\n",
    "            print(f\"Loading {category}...\")\n",
    "            df = spark.read.json(f\"{base_path}{category}.filtered.json\")\n",
    "            df = df.withColumn(\"category\", F.lit(category))\n",
    "            all_df = all_df.union(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {category}: {str(e)}\")\n",
    "    \n",
    "    return all_df\n",
    "\n",
    "# Load data from all categories (this might take a while)\n",
    "all_reviews = load_categories(categories, base_path)\n",
    "print(f\"Total reviews loaded: {all_reviews.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378230b-f7a1-4f78-897a-ae8f817087e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# First, combine the review summary and text\n",
    "all_reviews = all_reviews.withColumn(\n",
    "    \"full_text\", \n",
    "    F.concat_ws(\" \", \n",
    "                F.col(\"Review/Summary\"), \n",
    "                F.col(\"Review/Text\"))\n",
    ")\n",
    "\n",
    "# Step 1: Tokenize the text (split into words)\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"full_text\", \n",
    "    outputCol=\"words\", \n",
    "    pattern=\"\\\\W+\"  # Split on non-word characters\n",
    ")\n",
    "reviews_tokenized = tokenizer.transform(all_reviews)\n",
    "\n",
    "# Step 2: Remove stopwords (common words like \"the\", \"and\", etc.)\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"words\", \n",
    "    outputCol=\"filtered_words\"\n",
    ")\n",
    "reviews_no_stopwords = remover.transform(reviews_tokenized)\n",
    "\n",
    "# Step 3: Filter out very short words\n",
    "filter_short_udf = F.udf(\n",
    "    lambda words: [word for word in words if len(word) > 2],\n",
    "    ArrayType(StringType())\n",
    ")\n",
    "processed_reviews = reviews_no_stopwords.withColumn(\n",
    "    \"filtered_words\",\n",
    "    filter_short_udf(F.col(\"filtered_words\"))\n",
    ")\n",
    "\n",
    "# Check our processing results\n",
    "processed_reviews.select(\"category\", \"full_text\", \"filtered_words\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ad0975-d952-4e9b-95f0-2a63e062ef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer to convert words to vectors\n",
    "# This counts how many times each word appears\n",
    "vectorizer = CountVectorizer(\n",
    "    inputCol=\"filtered_words\", \n",
    "    outputCol=\"features\",\n",
    "    vocabSize=10000,  # Keep the top 10,000 words\n",
    "    minDF=5           # Word must appear in at least 5 documents\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on our data\n",
    "vectorizer_model = vectorizer.fit(processed_reviews)\n",
    "vectorized_reviews = vectorizer_model.transform(processed_reviews)\n",
    "\n",
    "# See what we get\n",
    "vectorized_reviews.select(\"category\", \"filtered_words\", \"features\").show(2)\n",
    "\n",
    "# Get the vocabulary for later use\n",
    "vocabulary = vectorizer_model.vocabulary\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Some example words: {vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f125e-8e3a-456d-a0cc-469db5d3971f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Set the number of topics (this is a key parameter!)\n",
    "num_topics = 20\n",
    "\n",
    "# Create the LDA model\n",
    "lda = LDA(\n",
    "    k=num_topics,           # Number of topics\n",
    "    maxIter=20,             # Number of iterations\n",
    "    featuresCol=\"features\", # Column with our word vectors\n",
    "    optimizer=\"em\"          # Use expectation-maximization algorithm\n",
    ")\n",
    "\n",
    "# Train the model (this might take a while!)\n",
    "print(\"Training LDA model...\")\n",
    "lda_model = lda.fit(vectorized_reviews)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783accd-536c-41fc-a0b9-a1dcf8f35443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Get topics with their top terms\n",
    "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
    "\n",
    "# Convert term indices to actual words\n",
    "def term_indices_to_words(indices):\n",
    "    return [vocabulary[idx] for idx in indices]\n",
    "\n",
    "# Create a UDF (User-Defined Function) for this\n",
    "term_indices_to_words_udf = F.udf(term_indices_to_words, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to get readable topics\n",
    "topics_with_words = topics.withColumn(\n",
    "    \"terms\", \n",
    "    term_indices_to_words_udf(F.col(\"termIndices\"))\n",
    ")\n",
    "\n",
    "# Show each topic with its top terms\n",
    "topics_with_words.select(\"topic\", \"terms\", \"termWeights\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae681c-819b-4675-bca6-6fcf3c975b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Apply the model to get topic distributions for each review\n",
    "reviews_with_topics = lda_model.transform(vectorized_reviews)\n",
    "\n",
    "# Extract the primary topic for each review\n",
    "def get_primary_topic(distribution):\n",
    "    return float(distribution.argmax())\n",
    "\n",
    "# Create a UDF for this\n",
    "get_primary_topic_udf = F.udf(get_primary_topic, FloatType())\n",
    "\n",
    "# Get the primary topic for each review\n",
    "reviews_with_topics = reviews_with_topics.withColumn(\n",
    "    \"primary_topic\", \n",
    "    get_primary_topic_udf(F.col(\"topicDistribution\"))\n",
    ")\n",
    "\n",
    "# Check distribution of topics\n",
    "reviews_with_topics.groupBy(\"primary_topic\").count().orderBy(\"primary_topic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b4324-7385-4ab4-9719-6d02aaff28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution by category\n",
    "topic_by_category = reviews_with_topics.groupBy(\"category\", \"primary_topic\").count()\n",
    "\n",
    "# Calculate percentages within each category\n",
    "category_totals = reviews_with_topics.groupBy(\"category\").count().withColumnRenamed(\"count\", \"total\")\n",
    "\n",
    "# Join the counts with totals\n",
    "topic_percentage = topic_by_category.join(category_totals, on=\"category\")\n",
    "topic_percentage = topic_percentage.withColumn(\n",
    "    \"percentage\", \n",
    "    F.round((F.col(\"count\") / F.col(\"total\") * 100), 2)\n",
    ")\n",
    "\n",
    "# Show results sorted by category and percentage\n",
    "topic_percentage.orderBy(\"category\", F.desc(\"percentage\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9afc0-4113-4e3a-a7f6-a330bb8fe47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for easier manipulation\n",
    "topics_df = topics_with_words.toPandas()\n",
    "\n",
    "print(\"TOPIC SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, row in topics_df.iterrows():\n",
    "    topic_id = row['topic']\n",
    "    terms = row['terms']\n",
    "    weights = row['termWeights']\n",
    "    \n",
    "    # Sort terms by weight\n",
    "    term_weights = sorted(zip(terms, weights), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display top terms with weights\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\", \".join([f\"{term} ({weight:.3f})\" for term, weight in term_weights[:10]]))\n",
    "    \n",
    "    # Show what categories this topic appears in most\n",
    "    top_categories = topic_percentage.filter(F.col(\"primary_topic\") == topic_id) \\\n",
    "                                      .orderBy(F.desc(\"percentage\")) \\\n",
    "                                      .limit(3)\n",
    "    \n",
    "    top_categories_pd = top_categories.toPandas()\n",
    "    if not top_categories_pd.empty:\n",
    "        category_info = \", \".join([f\"{row['category']} ({row['percentage']}%)\" \n",
    "                                  for _, row in top_categories_pd.iterrows()])\n",
    "        print(f\"Most common in: {category_info}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c79f7-e9ab-4e5b-b82f-628e21eefea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics for reference\n",
    "topics_with_words.write.mode(\"overwrite\").parquet(\n",
    "    \"hdfs://namenode:9000/user/hadoop/amazon_reviews/results/lda_topics\"\n",
    ")\n",
    "\n",
    "# Save review topic assignments\n",
    "reviews_with_topics.select(\n",
    "    \"ProductID\", \"category\", \"Review/Score\", \"primary_topic\", \"topicDistribution\"\n",
    ").write.mode(\"overwrite\").parquet(\n",
    "    \"hdfs://namenode:9000/user/hadoop/amazon_reviews/results/review_topics\"\n",
    ")\n",
    "\n",
    "print(\"Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9abda-6e65-4bd4-bcb9-4be62bdf5464",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797449ed-2bd8-46b4-a343-48028e3b91de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.clustering import LDA\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "def process_one_category(category):\n",
    "    print(f\"Starting to process: {category}\")\n",
    "    \n",
    "    # Step 3.1: Load just this category's data\n",
    "    try:\n",
    "        df = spark.read.json(f\"{base_path}{category}.filtered.json\")\n",
    "        print(f\"  Loaded {df.count()} reviews for {category}\")\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR loading {category}: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Step 3.2: Combine review title and text\n",
    "    df = df.withColumn(\n",
    "        \"full_text\", \n",
    "        F.concat_ws(\" \", F.col(\"Review/Summary\"), F.col(\"Review/Text\"))\n",
    "    )\n",
    "    \n",
    "    # Step 3.3: Tokenize (split text into words)\n",
    "    tokenizer = RegexTokenizer(\n",
    "        inputCol=\"full_text\", \n",
    "        outputCol=\"words\", \n",
    "        pattern=\"\\\\W+\"  # Split on non-word characters\n",
    "    )\n",
    "    df_tokenized = tokenizer.transform(df)\n",
    "    \n",
    "    # Step 3.4: Remove stopwords\n",
    "    remover = StopWordsRemover(\n",
    "        inputCol=\"words\", \n",
    "        outputCol=\"filtered_words\"\n",
    "    )\n",
    "    df_filtered = remover.transform(df_tokenized)\n",
    "    \n",
    "    # Step 3.5: Remove short words (less than 3 chars)\n",
    "    filter_short_udf = F.udf(\n",
    "        lambda words: [word for word in words if len(word) > 2],\n",
    "        ArrayType(StringType())\n",
    "    )\n",
    "    df_filtered = df_filtered.withColumn(\n",
    "        \"filtered_words\",\n",
    "        filter_short_udf(F.col(\"filtered_words\"))\n",
    "    )\n",
    "    \n",
    "    # Step 3.6: Convert words to vectors\n",
    "    vectorizer = CountVectorizer(\n",
    "        inputCol=\"filtered_words\", \n",
    "        outputCol=\"features\",\n",
    "        vocabSize=5000,  # Smaller vocabulary to save memory\n",
    "        minDF=3          # Word must appear in at least 3 documents\n",
    "    )\n",
    "    \n",
    "    # Fit and transform\n",
    "    vectorizer_model = vectorizer.fit(df_filtered)\n",
    "    df_vectorized = vectorizer_model.transform(df_filtered)\n",
    "    \n",
    "    # Get vocabulary for later use\n",
    "    vocabulary = vectorizer_model.vocabulary\n",
    "    print(f\"  Created vocabulary with {len(vocabulary)} words\")\n",
    "    \n",
    "    # Step 3.7: Run LDA\n",
    "    num_topics = 10  # Fewer topics to save memory\n",
    "    \n",
    "    lda = LDA(\n",
    "        k=num_topics,\n",
    "        maxIter=10,    # Fewer iterations to save memory\n",
    "        featuresCol=\"features\",\n",
    "        optimizer=\"online\"  # More memory-efficient\n",
    "    )\n",
    "    \n",
    "    # Train the model\n",
    "    print(f\"  Training LDA model for {category}...\")\n",
    "    try:\n",
    "        lda_model = lda.fit(df_vectorized)\n",
    "        print(f\"  LDA model for {category} complete!\")\n",
    "        \n",
    "        # Step 3.8: Get topics with their terms\n",
    "        topics = lda_model.describeTopics(maxTermsPerTopic=10)\n",
    "        \n",
    "        # Convert term indices to actual words\n",
    "        def indices_to_words(indices):\n",
    "            return [vocabulary[idx] for idx in indices]\n",
    "        \n",
    "        indices_to_words_udf = F.udf(indices_to_words, ArrayType(StringType()))\n",
    "        \n",
    "        # Apply the conversion\n",
    "        topics_with_words = topics.withColumn(\n",
    "            \"terms\", \n",
    "            indices_to_words_udf(F.col(\"termIndices\"))\n",
    "        )\n",
    "        \n",
    "        # Add category name\n",
    "        topics_with_words = topics_with_words.withColumn(\"category\", F.lit(category))\n",
    "        \n",
    "        # Step 3.9: Save the results\n",
    "\n",
    "        # Save model\n",
    "        model_path = f\"hdfs://namenode:9000/user/jovyan/amazon_reviews/models/lda_model_{category}\"\n",
    "        lda_model.write().overwrite().save(model_path)\n",
    "        print(f\"  Saved LDA model to {model_path}\")\n",
    "\n",
    "        # Save vocabulary (needed for the CountVectorizer)\n",
    "        vocab_rdd = spark.sparkContext.parallelize([(word,) for word in vocabulary])\n",
    "        vocab_df = spark.createDataFrame(vocab_rdd, [\"word\"])\n",
    "        vocab_df.write.mode(\"overwrite\").parquet(f\"hdfs://namenode:9000/user/jovyan/amazon_reviews/models/vocabulary_{category}\")\n",
    "\n",
    "        # 3. Save the CountVectorizer model\n",
    "        vectorizer_path = f\"hdfs://namenode:9000/user/jovyan/amazon_reviews/models/vectorizer_{category}\"\n",
    "        vectorizer_model.save(vectorizer_path)\n",
    "        print(f\"  Saved CountVectorizer model to {vectorizer_path}\")\n",
    "        \n",
    "        output_path = f\"hdfs://namenode:9000/user/jovyan/amazon_reviews/results/topics_{category}\"\n",
    "        topics_with_words.write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(f\"  Saved results to {output_path}\")\n",
    "        \n",
    "        # Return as pandas for local analysis\n",
    "        return topics_with_words.toPandas()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR in LDA for {category}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4814bbaf-fdfa-4f58-86e7-9f18897618bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Process each category\n",
    "for category in categories:\n",
    "    # Process this category\n",
    "    result = process_one_category(category)\n",
    "    \n",
    "    # Save result if successful\n",
    "    if result is not None:\n",
    "        results[category] = result\n",
    "        print(f\"Successfully processed {category}\")\n",
    "    else:\n",
    "        print(f\"Failed to process {category}\")\n",
    "    \n",
    "    # IMPORTANT: Clear cache between categories to free memory\n",
    "    spark.catalog.clearCache()\n",
    "    print(f\"Cleared cache after {category}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "print(\"All categories processed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c4ce108-dde5-477b-ade3-077c806884d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records: 18313\n",
      "Schema:\n",
      "+-------------+-----------------+----------------------------------------------+------------------+-------------------------------+------------+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|product/price|product/productId|product/title                                 |review/helpfulness|review/profileName             |review/score|review/summary               |review/text                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |review/time|review/userId |\n",
      "+-------------+-----------------+----------------------------------------------+------------------+-------------------------------+------------+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "|4.43         |B000G6HRZE       |Dritz(R) Extra Large Safety Pins-Size 3 65/Pkg|8/8               |M. Sarrao                      |3.0         |Not Very Sharp               |Like the previous reviewer, I was very happy to be able to order safety pins on-line. I live in a small town, and they are impossible to find. These safety pins are very large- about 2.25\" long. Also, they are not very sharp. I used them to pin 2-3 garments together for a children's clothing resale, and had a VERY difficult time getting the point to go through in both directions. This was true on all types of fabric from denim to cotton to jersey. The dull point of one of these safety pins tore a hole in one polo-style, jersey shirt!                                                                                                                                   |1269043200 |A2PXVMF9DH8NDZ|\n",
      "|4.43         |B000G6HRZE       |Dritz(R) Extra Large Safety Pins-Size 3 65/Pkg|5/5               |O. Brown \"Ms. O. Khannah-Brown\"|4.0         |Nice Large Basic Safety Pins |****These are your basic large safety pins and come in a package of 65. I always found myself buying assortments of safety pins and using up all of the large ones first, so these solve a problem for me. These safety pins are all the same size, called a \"Size 3\" as far as safety pin sizes go, but actually are two inches or 5.1 centimeters long. They are nickel-plated, not fancy, but do the job just fine. Save yourself a lot of time hunting these down in stores by buying them here on Amazon! They seem sharp enough for the uses I have for them; however, I don't think they would be good for any extra fine work, as they are just general-usage quality.Recommended.****|1321833600 |A1YUL9PCJR3JTY|\n",
      "|4.43         |B000G6HRZE       |Dritz(R) Extra Large Safety Pins-Size 3 65/Pkg|0/0               |Allyn C. Cornell               |5.0         |Great Safety Pins            |These Dritz(R) Extra Large Safety Pins are great. They are great for fixing hems and any other clothes emergencies. I highly recommend them.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |1360886400 |A2B6I0N8RNAQXJ|\n",
      "|4.43         |B000G6HRZE       |Dritz(R) Extra Large Safety Pins-Size 3 65/Pkg|0/0               |Judith A. Holland              |4.0         |When you need a safety pin...|We are organizing our costume stock and large quantity safety pins are in big demand. Wish this came in a larger quantity, but they do serve the purpose.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |1360627200 |A2FMZ7LFWTZCJ4|\n",
      "|4.43         |B000G6HRZE       |Dritz(R) Extra Large Safety Pins-Size 3 65/Pkg|0/0               |D. M. Albright                 |1.0         |Safety pins                  |As another reviewer noted these do not pierce cloth well at all so were basically useless for me. I threw them in the trash                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   |1358899200 |A1AUXDAAZR2VNT|\n",
      "+-------------+-----------------+----------------------------------------------+------------------+-------------------------------+------------+-----------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews Analysis\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "# Now you can read data from HDFS\n",
    "# Replace the path with your actual path where JSON files are stored\n",
    "df = spark.read.json(\"hdfs://namenode:9000/user/hadoop/amazon_reviews/data/filtered_data/Arts.filtered.json\")\n",
    "\n",
    "# Verify that data was loaded correctly\n",
    "print(f\"Number of records: {df.count()}\")\n",
    "print(\"Schema:\")\n",
    "#df.printSchema()\n",
    "\n",
    "# Show a few sample records\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c0406ab-119c-4685-be4f-96f34c6a70fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession created successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Initialize Spark with connection to your cluster and HDFS\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Amazon Reviews Topic Modeling\") \\\n",
    "    .master(\"spark://sparkmaster:8080\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"SparkSession created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d642094-cf5a-4bd7-bc3b-b86ccf65a9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading category: Amazon_Instant_Video\n",
      "Error loading category: Arts\n",
      "Error loading category: Automotive\n",
      "Error loading category: Baby\n",
      "Error loading category: Beauty\n",
      "Error loading category: Books\n",
      "Error loading category: Cell_Phones_&_Accessories\n",
      "Error loading category: Clothing_&_Accessories\n",
      "Error loading category: Electronics\n",
      "Error loading category: Gourmet_Foods\n",
      "Error loading category: Health\n",
      "Error loading category: Home_&_Kitchen\n",
      "Error loading category: Industrial_&_Scientific\n",
      "Error loading category: Jewelry\n",
      "Error loading category: Kindle_Store\n",
      "Error loading category: Movies_&_TV\n",
      "Error loading category: Music\n",
      "Error loading category: Musical_Instruments\n",
      "Error loading category: Office_Products\n",
      "Error loading category: Patio\n",
      "Error loading category: Pet_Supplies\n",
      "Error loading category: Shoes\n",
      "Error loading category: Software\n",
      "Error loading category: Sports_&_Outdoors\n",
      "Error loading category: Tools_&_Home_Improvement\n",
      "Error loading category: Toys_&_Games\n",
      "Error loading category: Video_Games\n",
      "Error loading category: Watches\n",
      "Total categories loaded: 0\n",
      "Total reviews: 0\n"
     ]
    }
   ],
   "source": [
    "# Adjust path to match your actual location in HDFS\n",
    "base_path = \"hdfs://namenode:9000/user/hadoop/amazon_reviews/data/filtered_data/\"\n",
    "\n",
    "# Get a count of reviews per category to understand data distribution\n",
    "category_counts = []\n",
    "\n",
    "# List all categories you have\n",
    "categories = [\n",
    "    \"Amazon_Instant_Video\",\n",
    "    \"Arts\",\n",
    "    \"Automotive\",\n",
    "    \"Baby\",\n",
    "    \"Beauty\",\n",
    "    \"Books\",\n",
    "    \"Cell_Phones_&_Accessories\",\n",
    "    \"Clothing_&_Accessories\",\n",
    "    \"Electronics\",\n",
    "    \"Gourmet_Foods\",\n",
    "    \"Health\",\n",
    "    \"Home_&_Kitchen\",\n",
    "    \"Industrial_&_Scientific\",\n",
    "    \"Jewelry\",\n",
    "    \"Kindle_Store\",\n",
    "    \"Movies_&_TV\",\n",
    "    \"Music\",\n",
    "    \"Musical_Instruments\",\n",
    "    \"Office_Products\",\n",
    "    \"Patio\",\n",
    "    \"Pet_Supplies\",\n",
    "    \"Shoes\",\n",
    "    \"Software\",\n",
    "    \"Sports_&_Outdoors\",\n",
    "    \"Tools_&_Home_Improvement\",\n",
    "    \"Toys_&_Games\",\n",
    "    \"Video_Games\",\n",
    "    \"Watches\"\n",
    "]\n",
    "\n",
    "# Count records per category\n",
    "for category in categories:\n",
    "    try:\n",
    "        df = spark.read.json(f\"{base_path}{category}.filtered.json\")\n",
    "        count = df.count()\n",
    "        category_counts.append((category, count))\n",
    "        print(f\"Category: {category}, Records: {count}\")\n",
    "    except:\n",
    "        print(f\"Error loading category: {category}\")\n",
    "\n",
    "# Check total number of reviews\n",
    "print(f\"Total categories loaded: {len(category_counts)}\")\n",
    "print(f\"Total reviews: {sum([count for _, count in category_counts])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e2981dd-aa62-427d-9949-ea86cf43c9e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Amazon_Instant_Video...\n",
      "Loading Arts...\n",
      "Loading Automotive...\n",
      "Loading Baby...\n",
      "Loading Beauty...\n",
      "Loading Books...\n",
      "Loading Cell_Phones_&_Accessories...\n",
      "Loading Clothing_&_Accessories...\n",
      "Loading Electronics...\n",
      "Loading Gourmet_Foods...\n",
      "Loading Health...\n",
      "Loading Home_&_Kitchen...\n",
      "Loading Industrial_&_Scientific...\n",
      "Loading Jewelry...\n",
      "Loading Kindle_Store...\n",
      "Loading Movies_&_TV...\n",
      "Loading Music...\n",
      "Loading Musical_Instruments...\n",
      "Loading Office_Products...\n",
      "Loading Patio...\n",
      "Loading Pet_Supplies...\n",
      "Loading Shoes...\n",
      "Loading Software...\n",
      "Loading Sports_&_Outdoors...\n",
      "Loading Tools_&_Home_Improvement...\n",
      "Loading Toys_&_Games...\n",
      "Loading Video_Games...\n",
      "Loading Watches...\n",
      "Total reviews loaded: 5315161\n"
     ]
    }
   ],
   "source": [
    "# Function to load all categories\n",
    "def load_categories(categories, base_path):\n",
    "    # Start with the first category\n",
    "    print(f\"Loading {categories[0]}...\")\n",
    "    all_df = spark.read.json(f\"{base_path}{categories[0]}.filtered.json\")\n",
    "    all_df = all_df.withColumn(\"category\", F.lit(categories[0]))\n",
    "    \n",
    "    # Add all other categories with a union\n",
    "    for category in categories[1:]:\n",
    "        try:\n",
    "            print(f\"Loading {category}...\")\n",
    "            df = spark.read.json(f\"{base_path}{category}.filtered.json\")\n",
    "            df = df.withColumn(\"category\", F.lit(category))\n",
    "            all_df = all_df.union(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error with {category}: {str(e)}\")\n",
    "    \n",
    "    return all_df\n",
    "\n",
    "# Load data from all categories (this might take a while)\n",
    "all_reviews = load_categories(categories, base_path)\n",
    "print(f\"Total reviews loaded: {all_reviews.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b378230b-f7a1-4f78-897a-ae8f817087e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|category            |full_text                                                                                                                                                                                                                 |filtered_words                                                                                                                                                                      |\n",
      "+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Amazon_Instant_Video|Jack Wagner Rocks Jack Wagner is excellent as he plays against type as the evil hitman in this NBC telefilm from 1988. Worth seeking out for Wagner's performance alone.                                                  |[jack, wagner, rocks, jack, wagner, excellent, plays, type, evil, hitman, nbc, telefilm, 1988, worth, seeking, wagner, performance, alone]                                          |\n",
      "|Amazon_Instant_Video|Moving Target I found this to be an intrieging suspense thriller with a minimum of violence and a happy ending. This is a rare combination in recent releases. I found it well excepted and enjoyed by teen age audiences.|[moving, target, found, intrieging, suspense, thriller, minimum, violence, happy, ending, rare, combination, recent, releases, found, well, excepted, enjoyed, teen, age, audiences]|\n",
      "+--------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# First, combine the review summary and text\n",
    "all_reviews = all_reviews.withColumn(\n",
    "    \"full_text\", \n",
    "    F.concat_ws(\" \", \n",
    "                F.col(\"Review/Summary\"), \n",
    "                F.col(\"Review/Text\"))\n",
    ")\n",
    "\n",
    "# Step 1: Tokenize the text (split into words)\n",
    "tokenizer = RegexTokenizer(\n",
    "    inputCol=\"full_text\", \n",
    "    outputCol=\"words\", \n",
    "    pattern=\"\\\\W+\"  # Split on non-word characters\n",
    ")\n",
    "reviews_tokenized = tokenizer.transform(all_reviews)\n",
    "\n",
    "# Step 2: Remove stopwords (common words like \"the\", \"and\", etc.)\n",
    "remover = StopWordsRemover(\n",
    "    inputCol=\"words\", \n",
    "    outputCol=\"filtered_words\"\n",
    ")\n",
    "reviews_no_stopwords = remover.transform(reviews_tokenized)\n",
    "\n",
    "# Step 3: Filter out very short words\n",
    "filter_short_udf = F.udf(\n",
    "    lambda words: [word for word in words if len(word) > 2],\n",
    "    ArrayType(StringType())\n",
    ")\n",
    "processed_reviews = reviews_no_stopwords.withColumn(\n",
    "    \"filtered_words\",\n",
    "    filter_short_udf(F.col(\"filtered_words\"))\n",
    ")\n",
    "\n",
    "# Check our processing results\n",
    "processed_reviews.select(\"category\", \"full_text\", \"filtered_words\").show(2, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ad0975-d952-4e9b-95f0-2a63e062ef1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            category|      filtered_words|            features|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|Amazon_Instant_Video|[jack, wagner, ro...|(10000,[62,91,264...|\n",
      "|Amazon_Instant_Video|[moving, target, ...|(10000,[5,61,180,...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Vocabulary size: 10000\n",
      "Some example words: ['one', 'book', 'great', 'like', 'good', 'well', 'time', 'get', 'really', 'also']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import CountVectorizer\n",
    "\n",
    "# Create a CountVectorizer to convert words to vectors\n",
    "# This counts how many times each word appears\n",
    "vectorizer = CountVectorizer(\n",
    "    inputCol=\"filtered_words\", \n",
    "    outputCol=\"features\",\n",
    "    vocabSize=10000,  # Keep the top 10,000 words\n",
    "    minDF=5           # Word must appear in at least 5 documents\n",
    ")\n",
    "\n",
    "# Fit the vectorizer on our data\n",
    "vectorizer_model = vectorizer.fit(processed_reviews)\n",
    "vectorized_reviews = vectorizer_model.transform(processed_reviews)\n",
    "\n",
    "# See what we get\n",
    "vectorized_reviews.select(\"category\", \"filtered_words\", \"features\").show(2)\n",
    "\n",
    "# Get the vocabulary for later use\n",
    "vocabulary = vectorizer_model.vocabulary\n",
    "print(f\"Vocabulary size: {len(vocabulary)}\")\n",
    "print(f\"Some example words: {vocabulary[:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d17f125e-8e3a-456d-a0cc-469db5d3971f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training LDA model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 44660)\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 295, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 267, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/accumulators.py\", line 271, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/serializers.py\", line 596, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n",
      "ERROR:root:Exception while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3526, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/tmp/ipykernel_6037/1346141551.py\", line 16, in <module>\n",
      "    lda_model = lda.fit(vectorized_reviews)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/ml/base.py\", line 205, in fit\n",
      "    return self._fit(dataset)\n",
      "           ^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 381, in _fit\n",
      "    java_model = self._fit_java(dataset)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/ml/wrapper.py\", line 378, in _fit_java\n",
      "    return self._java_obj.fit(dataset._jdf)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "                   ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: <exception str() failed>\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 516, in send_command\n",
      "    raise Py4JNetworkError(\"Answer from Java side is empty\")\n",
      "py4j.protocol.Py4JNetworkError: Answer from Java side is empty\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 539, in send_command\n",
      "    raise Py4JNetworkError(\n",
      "py4j.protocol.Py4JNetworkError: Error while sending or receiving\n"
     ]
    },
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 111] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[14], line 16\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining LDA model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m lda_model \u001b[38;5;241m=\u001b[39m \u001b[43mlda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorized_reviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[0;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/ml/wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31m<class 'str'>\u001b[0m: (<class 'ConnectionRefusedError'>, ConnectionRefusedError(111, 'Connection refused'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2131\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         traceback\u001b[38;5;241m.\u001b[39mprint_exc()\n\u001b[1;32m   2129\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 2131\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_showtraceback\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_pdb:\n\u001b[1;32m   2133\u001b[0m     \u001b[38;5;66;03m# drop into debugger\u001b[39;00m\n\u001b[1;32m   2134\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebugger(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/ipykernel/zmqshell.py:556\u001b[0m, in \u001b[0;36mZMQInteractiveShell._showtraceback\u001b[0;34m(self, etype, evalue, stb)\u001b[0m\n\u001b[1;32m    550\u001b[0m sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    551\u001b[0m sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m    553\u001b[0m exc_content \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraceback\u001b[39m\u001b[38;5;124m\"\u001b[39m: stb,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mename\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(etype\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m),\n\u001b[0;32m--> 556\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mevalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    557\u001b[0m }\n\u001b[1;32m    559\u001b[0m dh \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplayhook\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# Send exception info over pub socket for other clients than the caller\u001b[39;00m\n\u001b[1;32m    561\u001b[0m \u001b[38;5;66;03m# to pick up\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:471\u001b[0m, in \u001b[0;36mPy4JJavaError.__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    470\u001b[0m     gateway_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_exception\u001b[38;5;241m.\u001b[39m_gateway_client\n\u001b[0;32m--> 471\u001b[0m     answer \u001b[38;5;241m=\u001b[39m \u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    472\u001b[0m     return_value \u001b[38;5;241m=\u001b[39m get_return_value(answer, gateway_client, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m     \u001b[38;5;66;03m# Note: technically this should return a bytestring 'str' rather than\u001b[39;00m\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;66;03m# unicodes in Python 2; however, it can return unicodes for now.\u001b[39;00m\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;66;03m# See https://github.com/bartdag/py4j/issues/306 for more details.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 111] Connection refused"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Set the number of topics (this is a key parameter!)\n",
    "num_topics = 20\n",
    "\n",
    "# Create the LDA model\n",
    "lda = LDA(\n",
    "    k=num_topics,           # Number of topics\n",
    "    maxIter=20,             # Number of iterations\n",
    "    featuresCol=\"features\", # Column with our word vectors\n",
    "    optimizer=\"em\"          # Use expectation-maximization algorithm\n",
    ")\n",
    "\n",
    "# Train the model (this might take a while!)\n",
    "print(\"Training LDA model...\")\n",
    "lda_model = lda.fit(vectorized_reviews)\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783accd-536c-41fc-a0b9-a1dcf8f35443",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# Get topics with their top terms\n",
    "topics = lda_model.describeTopics(maxTermsPerTopic=15)\n",
    "\n",
    "# Convert term indices to actual words\n",
    "def term_indices_to_words(indices):\n",
    "    return [vocabulary[idx] for idx in indices]\n",
    "\n",
    "# Create a UDF (User-Defined Function) for this\n",
    "term_indices_to_words_udf = F.udf(term_indices_to_words, ArrayType(StringType()))\n",
    "\n",
    "# Apply the UDF to get readable topics\n",
    "topics_with_words = topics.withColumn(\n",
    "    \"terms\", \n",
    "    term_indices_to_words_udf(F.col(\"termIndices\"))\n",
    ")\n",
    "\n",
    "# Show each topic with its top terms\n",
    "topics_with_words.select(\"topic\", \"terms\", \"termWeights\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bae681c-819b-4675-bca6-6fcf3c975b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Apply the model to get topic distributions for each review\n",
    "reviews_with_topics = lda_model.transform(vectorized_reviews)\n",
    "\n",
    "# Extract the primary topic for each review\n",
    "def get_primary_topic(distribution):\n",
    "    return float(distribution.argmax())\n",
    "\n",
    "# Create a UDF for this\n",
    "get_primary_topic_udf = F.udf(get_primary_topic, FloatType())\n",
    "\n",
    "# Get the primary topic for each review\n",
    "reviews_with_topics = reviews_with_topics.withColumn(\n",
    "    \"primary_topic\", \n",
    "    get_primary_topic_udf(F.col(\"topicDistribution\"))\n",
    ")\n",
    "\n",
    "# Check distribution of topics\n",
    "reviews_with_topics.groupBy(\"primary_topic\").count().orderBy(\"primary_topic\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "581b4324-7385-4ab4-9719-6d02aaff28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get topic distribution by category\n",
    "topic_by_category = reviews_with_topics.groupBy(\"category\", \"primary_topic\").count()\n",
    "\n",
    "# Calculate percentages within each category\n",
    "category_totals = reviews_with_topics.groupBy(\"category\").count().withColumnRenamed(\"count\", \"total\")\n",
    "\n",
    "# Join the counts with totals\n",
    "topic_percentage = topic_by_category.join(category_totals, on=\"category\")\n",
    "topic_percentage = topic_percentage.withColumn(\n",
    "    \"percentage\", \n",
    "    F.round((F.col(\"count\") / F.col(\"total\") * 100), 2)\n",
    ")\n",
    "\n",
    "# Show results sorted by category and percentage\n",
    "topic_percentage.orderBy(\"category\", F.desc(\"percentage\")).show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb9afc0-4113-4e3a-a7f6-a330bb8fe47a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas for easier manipulation\n",
    "topics_df = topics_with_words.toPandas()\n",
    "\n",
    "print(\"TOPIC SUMMARY:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for i, row in topics_df.iterrows():\n",
    "    topic_id = row['topic']\n",
    "    terms = row['terms']\n",
    "    weights = row['termWeights']\n",
    "    \n",
    "    # Sort terms by weight\n",
    "    term_weights = sorted(zip(terms, weights), key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Display top terms with weights\n",
    "    print(f\"Topic {topic_id}:\")\n",
    "    print(\", \".join([f\"{term} ({weight:.3f})\" for term, weight in term_weights[:10]]))\n",
    "    \n",
    "    # Show what categories this topic appears in most\n",
    "    top_categories = topic_percentage.filter(F.col(\"primary_topic\") == topic_id) \\\n",
    "                                      .orderBy(F.desc(\"percentage\")) \\\n",
    "                                      .limit(3)\n",
    "    \n",
    "    top_categories_pd = top_categories.toPandas()\n",
    "    if not top_categories_pd.empty:\n",
    "        category_info = \", \".join([f\"{row['category']} ({row['percentage']}%)\" \n",
    "                                  for _, row in top_categories_pd.iterrows()])\n",
    "        print(f\"Most common in: {category_info}\")\n",
    "    \n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1c79f7-e9ab-4e5b-b82f-628e21eefea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save topics for reference\n",
    "topics_with_words.write.mode(\"overwrite\").parquet(\n",
    "    \"hdfs://namenode:9000/user/hadoop/amazon_reviews/results/lda_topics\"\n",
    ")\n",
    "\n",
    "# Save review topic assignments\n",
    "reviews_with_topics.select(\n",
    "    \"ProductID\", \"category\", \"Review/Score\", \"primary_topic\", \"topicDistribution\"\n",
    ").write.mode(\"overwrite\").parquet(\n",
    "    \"hdfs://namenode:9000/user/hadoop/amazon_reviews/results/review_topics\"\n",
    ")\n",
    "\n",
    "print(\"Results saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc9abda-6e65-4bd4-bcb9-4be62bdf5464",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
